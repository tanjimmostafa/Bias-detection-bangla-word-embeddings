{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tanjimmostafa/Bias-detection-bangla-word-embeddings/blob/main/WEAT_r.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RcvG7z3d7Z3s"
      },
      "source": [
        "# WEAT in Python2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UUSApiAF7Z3t"
      },
      "source": [
        "This notebook collects the basic functions of the WEAT procedure."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bb7rSe4b7Z3u"
      },
      "source": [
        "Here are the data sets that are used to define the assciation concepts. Here ist is only positive and negative. Sets are named after the terminology in each source paper. Yet, concepts are similar."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: pip install gensim\n",
        "\n",
        "!pip install gensim"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u4bmNbT8MgHz",
        "outputId": "37ddd30e-7917-41c1-99ec-481d02f6b78d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (4.3.2)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.25.2)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.11.4)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim) (6.4.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install -U bnlp_toolkit"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hH72XpbsYqcp",
        "outputId": "5368e67e-f571-45e7-b069-c50b7f97e070"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting bnlp_toolkit\n",
            "  Downloading bnlp_toolkit-4.0.1-py3-none-any.whl (22 kB)\n",
            "Collecting sentencepiece==0.2.0 (from bnlp_toolkit)\n",
            "  Downloading sentencepiece-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: gensim==4.3.2 in /usr/local/lib/python3.10/dist-packages (from bnlp_toolkit) (4.3.2)\n",
            "Requirement already satisfied: nltk==3.8.1 in /usr/local/lib/python3.10/dist-packages (from bnlp_toolkit) (3.8.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from bnlp_toolkit) (1.25.2)\n",
            "Collecting scipy==1.10.1 (from bnlp_toolkit)\n",
            "  Downloading scipy-1.10.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m34.4/34.4 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sklearn-crfsuite==0.3.6 (from bnlp_toolkit)\n",
            "  Downloading sklearn_crfsuite-0.3.6-py2.py3-none-any.whl (12 kB)\n",
            "Collecting tqdm==4.66.3 (from bnlp_toolkit)\n",
            "  Downloading tqdm-4.66.3-py3-none-any.whl (78 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.4/78.4 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting ftfy==6.2.0 (from bnlp_toolkit)\n",
            "  Downloading ftfy-6.2.0-py3-none-any.whl (54 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.4/54.4 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting emoji==1.7.0 (from bnlp_toolkit)\n",
            "  Downloading emoji-1.7.0.tar.gz (175 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m175.4/175.4 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from bnlp_toolkit) (2.31.0)\n",
            "Requirement already satisfied: wcwidth<0.3.0,>=0.2.12 in /usr/local/lib/python3.10/dist-packages (from ftfy==6.2.0->bnlp_toolkit) (0.2.13)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim==4.3.2->bnlp_toolkit) (6.4.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk==3.8.1->bnlp_toolkit) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk==3.8.1->bnlp_toolkit) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk==3.8.1->bnlp_toolkit) (2023.12.25)\n",
            "Collecting python-crfsuite>=0.8.3 (from sklearn-crfsuite==0.3.6->bnlp_toolkit)\n",
            "  Downloading python_crfsuite-0.9.10-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from sklearn-crfsuite==0.3.6->bnlp_toolkit) (1.16.0)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from sklearn-crfsuite==0.3.6->bnlp_toolkit) (0.9.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->bnlp_toolkit) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->bnlp_toolkit) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->bnlp_toolkit) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->bnlp_toolkit) (2024.2.2)\n",
            "Building wheels for collected packages: emoji\n",
            "  Building wheel for emoji (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for emoji: filename=emoji-1.7.0-py3-none-any.whl size=171034 sha256=0d655d3b73dd64a2192e257a3c91f49886c95749b7340b9c97d8d2de5e672c94\n",
            "  Stored in directory: /root/.cache/pip/wheels/31/8a/8c/315c9e5d7773f74b33d5ed33f075b49c6eaeb7cedbb86e2cf8\n",
            "Successfully built emoji\n",
            "Installing collected packages: sentencepiece, python-crfsuite, emoji, tqdm, scipy, ftfy, sklearn-crfsuite, bnlp_toolkit\n",
            "  Attempting uninstall: sentencepiece\n",
            "    Found existing installation: sentencepiece 0.1.99\n",
            "    Uninstalling sentencepiece-0.1.99:\n",
            "      Successfully uninstalled sentencepiece-0.1.99\n",
            "  Attempting uninstall: tqdm\n",
            "    Found existing installation: tqdm 4.66.4\n",
            "    Uninstalling tqdm-4.66.4:\n",
            "      Successfully uninstalled tqdm-4.66.4\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.11.4\n",
            "    Uninstalling scipy-1.11.4:\n",
            "      Successfully uninstalled scipy-1.11.4\n",
            "Successfully installed bnlp_toolkit-4.0.1 emoji-1.7.0 ftfy-6.2.0 python-crfsuite-0.9.10 scipy-1.10.1 sentencepiece-0.2.0 sklearn-crfsuite-0.3.6 tqdm-4.66.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: locate in google drive\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "%cd '/content/drive/My Drive/Colab Notebooks'\n",
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HfkwBeCFNcuH",
        "outputId": "16cd78ca-fb35-4583-e9a8-c6efc2a0e138"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive/My Drive/Colab Notebooks\n",
            "bnwiki_word2vec.model\t\t    models\t\t      util.py\n",
            "foo.txt\t\t\t\t    __pycache__\t\t      WEAT-r.ipynb\n",
            "GoogleNews-vectors-negative300.bin  task-spamfiltering.ipynb  wordSets.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.append('/content/drive/My Drive/Colab Notebooks/util.py')\n",
        "import util"
      ],
      "metadata": {
        "id": "nC9LSXJDOf1m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.append('/content/drive/My Drive/Colab Notebooks/wordSets.py')\n",
        "import wordSets"
      ],
      "metadata": {
        "id": "An5LxfB3OuJh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nFNmarvq7Z3u"
      },
      "outputs": [],
      "source": [
        "# Association sets from IAT literature\n",
        "\n",
        "# Greenwald et al.(1998) - Measuring Individual Differences In Implicit Cognition - The Implicit Association Test\n",
        "gw_pos = ['caress', 'freedom', 'health', 'love', 'peace', 'cheer', 'friend', 'heaven', 'loyal', 'pleasure',\n",
        "          'diamond', 'gentle', 'honest', 'lucky', 'rainbow', 'diploma', 'gift', 'honor', 'miracle', 'sunrise',\n",
        "          'family', 'happy', 'laughter', 'paradise', 'vacation']\n",
        "\n",
        "gw_neg = ('abuse crash filth murder sickness accident death grief poison stink assault disaster hatred pollute '\n",
        "          'tragedy bomb divorce jail poverty ugly cancer evil kill rotten vomit agony prison').split()\n",
        "\n",
        "# Nosek, Banaji, Greendwald (2002) - Math Male Me Female Therefore Me Not Math\n",
        "Pleasant_1 = 'assertive athletic strong compassion support sympathetic laughter champion paradise vacation'.split()\n",
        "Unpleasant_1 = 'brutal destroy ruthless confusion insecure naive bad poor waste crude'.split()\n",
        "\n",
        "Pleasant_2 = 'ambition cuddle excitement glory joy love paradise pleasure romantic miracle'.split()\n",
        "Unpleasant_2 = 'agony death detest disaster humiliate jealousy punishment stress tragedy war'.split()\n",
        "\n",
        "Pleasant_3 = 'affectionate cozy enjoyment friend hug laughter passion peace snuggle triumph'.split()\n",
        "Unpleasant_3 = 'afraid crucify despise failure hatred irritate nightmare slap terrible violent'.split()\n",
        "\n",
        "# Nosek et al. (2002) - Harvesting  implicit  group  attitudes  and  beliefs  from  a demonstration  web  site\n",
        "harvest_good = 'Joy Love Peace Wonderful Pleasure Friend Laughter Happy'.lower().split()\n",
        "harvest_bad = 'Agony Terrible Horrible Nasty Evil War Awful Failure Death'.lower().split()\n",
        "\n",
        "# Monteith & Pettit (2011) - Implicit and explicit  stigmatizing  attitudes  and  stereotypes  about  depression.\n",
        "mp_good = 'positive pleasant enjoy glorious wonderful bliss'.split()\n",
        "mp_bad = 'negative horrible agony terrible unpleasant despise'.split()\n",
        "\n",
        "###\n",
        "generalPos = set(Pleasant_1 + Pleasant_2 + Pleasant_3 + gw_pos + harvest_good + mp_good)\n",
        "generalNeg = set(Unpleasant_1 + Unpleasant_2 + Unpleasant_3 + gw_neg + harvest_bad + mp_bad)\n",
        "\n",
        "# further word sets:\n",
        "from wordSets import *"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#For bnlp bn_wiki\n",
        "\n",
        "!mkdir models\n",
        "%cd models"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zuZ_ec1F9cI2",
        "outputId": "7f844235-8a24-481f-eb3b-1200f6494592"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘models’: File exists\n",
            "/content/drive/MyDrive/Colab Notebooks/models\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://huggingface.co/sagorsarker/bangla_word2vec/resolve/main/bangla_word2vec_gen4.zip\n",
        "!unzip bangla_word2vec_gen4.zip\n",
        "!rm -rf bangla_word2vec_gen4.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "krXJAZW19inp",
        "outputId": "79f53d08-6546-4d12-dd49-d4d73d4f7812"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-04-29 10:08:08--  https://huggingface.co/sagorsarker/bangla_word2vec/resolve/main/bangla_word2vec_gen4.zip\n",
            "Resolving huggingface.co (huggingface.co)... 65.8.178.93, 65.8.178.12, 65.8.178.27, ...\n",
            "Connecting to huggingface.co (huggingface.co)|65.8.178.93|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://cdn-lfs.huggingface.co/repos/f5/12/f51294acf565f8e7f2b231a61a327e6a79fce680615e3bbbc4850f166e928650/b157fe019ddda848f0195240f636ff2dcb63cbe823c84eed77c129e196105fd1?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27bangla_word2vec_gen4.zip%3B+filename%3D%22bangla_word2vec_gen4.zip%22%3B&response-content-type=application%2Fzip&Expires=1714644488&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcxNDY0NDQ4OH19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5odWdnaW5nZmFjZS5jby9yZXBvcy9mNS8xMi9mNTEyOTRhY2Y1NjVmOGU3ZjJiMjMxYTYxYTMyN2U2YTc5ZmNlNjgwNjE1ZTNiYmJjNDg1MGYxNjZlOTI4NjUwL2IxNTdmZTAxOWRkZGE4NDhmMDE5NTI0MGY2MzZmZjJkY2I2M2NiZTgyM2M4NGVlZDc3YzEyOWUxOTYxMDVmZDE%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qJnJlc3BvbnNlLWNvbnRlbnQtdHlwZT0qIn1dfQ__&Signature=rCjmSRvpytFj9uppXvbYTl2szoN7VxYRRWKcchyL5suMwlq2ZWczVYY8ElwqjhTEEX%7EFXIiViMhfgQmBQq%7E8wMgj0zryAtWImVDoui7LQUWhSLHe-goGDDff-5mmBI2mIqJpsGP%7EJgkaPvvWF4zHAV0SyHgBxF7VKvG8z1WuWnHf1%7EhxQRUYe40kTiO8ZBkjmKXs1fZNdpyTRwbV3EAtENXrMx7xDAzhbEKFClCyNleGIpQ9E6ELIiRkid3201ByP0JUox%7ETl2I6Bu6o2CzfbDG2cgjnQTgmMgxOIKDpJndduKZ6Xb9pE7wU%7E0UkD%7EbpyZUZdsuVvcVZA9dvW629vg__&Key-Pair-Id=KVTP0A1DKRTAX [following]\n",
            "--2024-04-29 10:08:09--  https://cdn-lfs.huggingface.co/repos/f5/12/f51294acf565f8e7f2b231a61a327e6a79fce680615e3bbbc4850f166e928650/b157fe019ddda848f0195240f636ff2dcb63cbe823c84eed77c129e196105fd1?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27bangla_word2vec_gen4.zip%3B+filename%3D%22bangla_word2vec_gen4.zip%22%3B&response-content-type=application%2Fzip&Expires=1714644488&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcxNDY0NDQ4OH19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5odWdnaW5nZmFjZS5jby9yZXBvcy9mNS8xMi9mNTEyOTRhY2Y1NjVmOGU3ZjJiMjMxYTYxYTMyN2U2YTc5ZmNlNjgwNjE1ZTNiYmJjNDg1MGYxNjZlOTI4NjUwL2IxNTdmZTAxOWRkZGE4NDhmMDE5NTI0MGY2MzZmZjJkY2I2M2NiZTgyM2M4NGVlZDc3YzEyOWUxOTYxMDVmZDE%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qJnJlc3BvbnNlLWNvbnRlbnQtdHlwZT0qIn1dfQ__&Signature=rCjmSRvpytFj9uppXvbYTl2szoN7VxYRRWKcchyL5suMwlq2ZWczVYY8ElwqjhTEEX%7EFXIiViMhfgQmBQq%7E8wMgj0zryAtWImVDoui7LQUWhSLHe-goGDDff-5mmBI2mIqJpsGP%7EJgkaPvvWF4zHAV0SyHgBxF7VKvG8z1WuWnHf1%7EhxQRUYe40kTiO8ZBkjmKXs1fZNdpyTRwbV3EAtENXrMx7xDAzhbEKFClCyNleGIpQ9E6ELIiRkid3201ByP0JUox%7ETl2I6Bu6o2CzfbDG2cgjnQTgmMgxOIKDpJndduKZ6Xb9pE7wU%7E0UkD%7EbpyZUZdsuVvcVZA9dvW629vg__&Key-Pair-Id=KVTP0A1DKRTAX\n",
            "Resolving cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)... 108.157.173.84, 108.157.173.44, 108.157.173.105, ...\n",
            "Connecting to cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)|108.157.173.84|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 199591017 (190M) [application/zip]\n",
            "Saving to: ‘bangla_word2vec_gen4.zip’\n",
            "\n",
            "bangla_word2vec_gen 100%[===================>] 190.34M  41.0MB/s    in 4.8s    \n",
            "\n",
            "2024-04-29 10:08:14 (40.0 MB/s) - ‘bangla_word2vec_gen4.zip’ saved [199591017/199591017]\n",
            "\n",
            "Archive:  bangla_word2vec_gen4.zip\n",
            "replace bangla_word2vec/README.md? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n",
            "replace bangla_word2vec/bnwiki_word2vec.model? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n",
            "replace bangla_word2vec/bnwiki_word2vec.model.syn1neg.npy? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n",
            "replace bangla_word2vec/bnwiki_word2vec.model.wv.vectors.npy? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n",
            "replace bangla_word2vec/bnwiki_word2vec.vector? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n",
            "replace bangla_word2vec/evaluations/evaluation.json? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uurMlo7w7Z3v"
      },
      "source": [
        "Start with some imports of course ..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qVuVagV67Z3v"
      },
      "outputs": [],
      "source": [
        "#from bnlp import BengaliWord2Vec as bwv\n",
        "from gensim.models import KeyedVectors\n",
        "from gensim.models import Word2Vec\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy\n",
        "import math\n",
        "import logging, os\n",
        "from util import *\n",
        "# logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mzENKQHH7Z3w"
      },
      "source": [
        "Load pretrained W2V Google Model.\n",
        "The models can be downloaded at https://github.com/mmihaltz/word2vec-GoogleNews-vectors and https://github.com/eyaler/word2vec-slim, respectively. In most cases, the slim version is more than sufficient and much faster."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_STB38H37Z3w"
      },
      "outputs": [],
      "source": [
        "#m = KeyedVectors.load_word2vec_format('./GoogleNews-vectors-negative300.bin', binary=True)\n",
        "#m = KeyedVectors.load_word2vec_format('/content/drive/My Drive/Colab Notebooks/GoogleNews-vectors-negative300.bin', binary=True)\n",
        "m = Word2Vec.load('/content/drive/My Drive/Colab Notebooks/models/bangla_word2vec/bnwiki_word2vec.model')\n",
        "#m = textblob.Word2Vec.load('/content/drive/My Drive/Colab Notebooks/bnwiki_word2vec.model')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: check codec type of this file\n",
        "\n",
        "!file -bi /content/drive/My Drive/Colab Notebooks/models/bangla_word2vec/bnwiki_word2vec.model\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HRQ1NULHhHUk",
        "outputId": "505127a6-b862-4ad5-b858-6ead5826d42b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cannot open `/content/drive/My' (No such file or directory)\n",
            "cannot open `Drive/Colab' (No such file or directory)\n",
            "cannot open `Notebooks/models/bangla_word2vec/bnwiki_word2vec.model' (No such file or directory)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FM1px_W67Z3w"
      },
      "source": [
        "Here are the basic functions. Functions are kept as modular as possible so that they can be replaced and manipulated straightforwardly."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import gensim\n",
        "# from bnlp import BengaliWord2Vec as bwv\n",
        " # bwv = BengaliWord2Vec()\n",
        "\n",
        "# model_path = \"/content/drive/My Drive/Colab Notebooks/models/bangla_word2vec/bnwiki_word2vec.model\"\n",
        "# m = gensim.models.Word2Vec.load(model_path)\n",
        "# methods = dir(bwv)\n",
        "# print(methods)\n",
        "\n",
        "# word = 'গ্রাম'\n",
        "# vector = bwv.get_most_similar_words(m, word)\n",
        "# print(vector)\n",
        "\n",
        "# print(m.wv['গ্রাম'])"
      ],
      "metadata": {
        "id": "N8ZS5ptlZIyo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get most similar words to গ্রাম\n",
        "\n",
        "vector = m.wv.most_similar('গ্রাম')\n",
        "print(vector)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xp31yVdIA7CV",
        "outputId": "59711832-b041-4143-cd55-978f7925cb8c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('মৌজা', 0.7147208452224731), ('মহল্লা', 0.6878851652145386), ('তালুক', 0.6774986386299133), ('গ্রামের', 0.6585026383399963), ('তহসিল', 0.6413203477859497), ('পুরসভা', 0.6314382553100586), ('গ্রামে', 0.6264088749885559), ('সংগ্রামপুর', 0.6029990315437317), ('গ্রামাঞ্চল', 0.5972400307655334), ('মৌজার', 0.5972233414649963)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tFiuqg1GUKZq"
      },
      "outputs": [],
      "source": [
        "def get_cosines(model, word, word_list):\n",
        "    # Convert the Word2Vec object to a list of words\n",
        "    words = list(model.wv.key_to_index)\n",
        "\n",
        "    \"\"\"\n",
        "    Calculate the cosine-similarity of a word to every word in a word list.\n",
        "    Here it is the mean. May also be replaced by other metrices.\n",
        "    :param model:       underlying model\n",
        "    :param word:        target word\n",
        "    :param word_list:   list of words to calculate distance to\n",
        "    :return:            list of cosine similarities, mean cosine similarity\n",
        "    \"\"\"\n",
        "\n",
        "    cosines = []\n",
        "    for elem in word_list:\n",
        "        try:\n",
        "            cosines.append(model.similarity(word, elem))    # Similarity: Compute cosine similarity between two words.\n",
        "        except KeyError:\n",
        "            logging.info('no cosine for ' + word + ' and ' + elem + ' available')\n",
        "\n",
        "    if not cosines:       # Fehler abfangen\n",
        "        logging.error('No cosine values available')\n",
        "        return 0, 0\n",
        "\n",
        "    mean_cosine = sum(cosines) / len(cosines)\n",
        "    return cosines, mean_cosine\n",
        "\n",
        "\n",
        "def s_word(model, w, A, B, out):\n",
        "    # Convert the Word2Vec object to a list of words\n",
        "    words = list(model.wv.index_to_key)\n",
        "    \"\"\"\n",
        "    Calculate the association of w with the attribute sets (Is w rather associated to A (positive value) or to B (negative value))\n",
        "    :param model:       underlying model\n",
        "    :param w:           target word\n",
        "    :param A:           association set 1\n",
        "    :param B:           association set 2\n",
        "    :param out:         (boolean) only do prints if true\n",
        "    :return:            s-value\n",
        "    \"\"\"\n",
        "    # global words\n",
        "\n",
        "    cosines_wA, mean_cos_wA = get_cosines(model, w, A)\n",
        "    cosines_wB, mean_cos_wB = get_cosines(model, w, B)\n",
        "\n",
        "    s = 0\n",
        "    s_word_val = mean_cos_wA - mean_cos_wB          # negativ, wenn w zu B gehört, sonst positiv\n",
        "    s += s_word_val\n",
        "    if out:\n",
        "        assignment = 'failed (0)'\n",
        "        if s_word_val > 0:\n",
        "            assignment = 'A (' + format(s_word_val, '.4f') + ')'\n",
        "        elif s_word_val < 0:\n",
        "            assignment = 'B (' + format(s_word_val, '.4f') + ')'\n",
        "\n",
        "        info = w + ': A = ' + format(mean_cos_wA, '.4f') + ' ; B = ' \\\n",
        "               + format(mean_cos_wB, '.4f') + ' | assignment: ' + assignment\n",
        "        print(info)\n",
        "    return s_word_val\n",
        "\n",
        "\n",
        "# not  used\n",
        "def s(model, X, Y, A, B, out):\n",
        "    # Convert the Word2Vec object to a list of words\n",
        "    words = list(model.wv.index_to_key)\n",
        "\n",
        "    \"\"\"\n",
        "    Calculate the differential association of the two sets of target group with the attribute. This is exactly the same\n",
        "    function as \"s_corrected\" but without normalisation. Thus, this function is no longer used by default.\n",
        "    :param model:       underlying model\n",
        "    :param X:           target set 1\n",
        "    :param Y:           target set 2\n",
        "    :param A:           association set 1\n",
        "    :param B:           association set 2\n",
        "    :param out:         only do prints if true\n",
        "    :return:            s value\n",
        "    \"\"\"\n",
        "\n",
        "    # global words\n",
        "\n",
        "    s_xAB_all = []\n",
        "    s_yAB_all = []\n",
        "\n",
        "    for x in X:\n",
        "        if x not in model:\n",
        "            logging.info(x + ' is not in vocabulary -> skip it')\n",
        "        else:\n",
        "            curr = s_word(model, x, A, B, out)             # im Optimalfall positiv\n",
        "            s_xAB_all.append(curr)\n",
        "    if out:\n",
        "        print('~~~~~~~~~~*~~~~~~~~~~*~~~~~~~~~~*~~~~~~~~~~*~~~~~~~~~~*~~~~~~~~~~')\n",
        "\n",
        "    for y in Y:\n",
        "        if y not in words:\n",
        "            logging.info(y + ' is not in vocabulary -> skip it')\n",
        "        else:\n",
        "            curr = s_word(words, y, A, B, out)             # im Optimalfall negativ\n",
        "            s_yAB_all.append(curr)\n",
        "\n",
        "    if not s_xAB_all or not s_yAB_all:          # Fehler abfangen\n",
        "        logging.error('none of the words is in vocabulary')\n",
        "    sum_sXAB = sum(s_xAB_all)                   # -> Summe positiv\n",
        "    sum_sYAB = sum(s_yAB_all)                   # -> Summe negativ\n",
        "\n",
        "    return sum_sXAB - sum_sYAB      # im Optimalfall ein hoher wert, da zwei Mal -\n",
        "\n",
        "\n",
        "def s_corrected(model, X, Y, A, B, out):\n",
        "    # Convert the Word2Vec object to a list of words\n",
        "    words = list(model.wv.index_to_key)\n",
        "\n",
        "    \"\"\"\n",
        "    Calculate the differential association of the two sets of target group with the attribute.\n",
        "    This function is a corrected and thus updated version of s(model, X, Y, A, B, out).\n",
        "    Normalisation allows to insert target word groups with different numbers of elements.\n",
        "    :param model:       underlying model\n",
        "    :param X:           target set 1\n",
        "    :param Y:           target set 2\n",
        "    :param A:           association set 1\n",
        "    :param B:           association set 2\n",
        "    :param out:         only do prints if true\n",
        "    :return:            s value\n",
        "    \"\"\"\n",
        "    # global words\n",
        "\n",
        "    s_xAB_all = []\n",
        "    s_yAB_all = []\n",
        "\n",
        "    for x in X:\n",
        "        if x not in words:\n",
        "            logging.info(x + ' in vocabulary (s, x): ' + str(x in words))\n",
        "            print(x, words, x in words)\n",
        "        else:\n",
        "            curr = s_word(words, x, A, B, out)          # im Optimalfall positiv\n",
        "            s_xAB_all.append(curr)\n",
        "    if out:\n",
        "        print('~~~~~~~~~~*~~~~~~~~~~*~~~~~~~~~~*~~~~~~~~~~*~~~~~~~~~~*~~~~~~~~~~')\n",
        "\n",
        "    for y in Y:\n",
        "        if y not in words:\n",
        "            logging.info(y + ' in vocabulary (s, y): ' + str(y in words))\n",
        "            print(y, words, y in words)\n",
        "        else:\n",
        "            curr = s_word(words, y, A, B, out)          # im Optimalfall negativ\n",
        "            s_yAB_all.append(curr)\n",
        "\n",
        "    if not s_xAB_all or not s_yAB_all:                  # Fehler abfangen\n",
        "        logging.error('none of the words is in vocabulary')\n",
        "        return 0\n",
        "    sum_sXAB = sum(s_xAB_all) / len(X)                  # -> Summe positiv\n",
        "    sum_sYAB = sum(s_yAB_all) / len(Y)                  # -> Summe negativ\n",
        "\n",
        "    return sum_sXAB - sum_sYAB                          # im Optimalfall ein hoher wert, da zwei Mal -\n",
        "\n",
        "\n",
        "def effect_size(model, X, Y, A, B):\n",
        "    # Convert the Word2Vec object to a list of words\n",
        "    words = list(model.wv.index_to_key)\n",
        "\n",
        "    \"\"\"\n",
        "    calculate the effect size of association\n",
        "    :param X:           target set 1\n",
        "    :param Y:           target set 2\n",
        "    :param A:           association set 1\n",
        "    :param B:           association set 2\n",
        "    :param model:       underlying model\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    # global words\n",
        "\n",
        "    s_values_x = []\n",
        "    s_values_y = []\n",
        "\n",
        "    for x in X:\n",
        "        if x not in words:\n",
        "            logging.info(x + ' in vocabulary (effect_size, x): ' + str(x in words))\n",
        "        else:\n",
        "            s_values_x.append(s_word(words, x, A, B, False))\n",
        "    for y in Y:\n",
        "        if y not in words:\n",
        "            logging.info(y + ' in vocabulary (effect_size, y): ' + str(y in words))\n",
        "        else:\n",
        "            s_values_y.append(s_word(words, y, A, B, False))\n",
        "\n",
        "    if not s_values_x or not s_values_y:                # Fehler abfangen\n",
        "        logging.error('non of the words is in vocabulary')\n",
        "        return 0\n",
        "\n",
        "    mean_s_val_x = sum(s_values_x) / len(s_values_x)\n",
        "    mean_s_val_y = sum(s_values_y) / len(s_values_y)\n",
        "\n",
        "    s_values_all = s_values_x + s_values_y\n",
        "    mean_s_val_all = sum(s_values_all) / len(s_values_all)\n",
        "    s_values_all_corrected = s_values_all\n",
        "    s_values_all_corrected[:] = [((x - mean_s_val_all)**2) for x in s_values_all]        # iterable: (x_i - mean(x))²\n",
        "    std_dev = math.sqrt((1 / (len(s_values_all) - 1)) * sum(s_values_all_corrected))\n",
        "\n",
        "    return (mean_s_val_x - mean_s_val_y) / std_dev\n",
        "\n",
        "\n",
        "def permutation_test(model, X, Y, A, B, tag, permutation_num):\n",
        "    # Convert the Word2Vec object to a list of words\n",
        "    words = list(model.wv.index_to_key)\n",
        "\n",
        "    \"\"\"\n",
        "    do s(X,Y,A,B) for all possible permutations of X & Y to check whether they are lower than weat\n",
        "    :param model:           underlying model\n",
        "    :param X:               target set 1\n",
        "    :param Y:               target set 2\n",
        "    :param A:               association set 1\n",
        "    :param B:               association set 2\n",
        "    :param tag:             (string) name for the saved logfile (no file extension)\n",
        "    :param permutation_num: Max number of random permutations\n",
        "    :return:                weat value, p value, number of permutations with higher weat value,\n",
        "                            ~ with equal ~, ~ with lower weat value\n",
        "    \"\"\"\n",
        "    # global words\n",
        "    all_target_words = X + Y\n",
        "    n = len(all_target_words)\n",
        "    print(n)\n",
        "    if (n%2) == 1:\n",
        "        logging.error('ungerade Anzahl von Target Words im Spiel!')\n",
        "    all_permutations = []\n",
        "\n",
        "\n",
        "    k = 0\n",
        "\n",
        "    full = False\n",
        "    if n <= 15:\n",
        "        full = True\n",
        "        logging.info('Do permutation test with full permutation.')\n",
        "        combinations = list(itertools.combinations(all_target_words, len(X)))\n",
        "        combinations = combinations[1:]               # remove second half and default order\n",
        "\n",
        "        controlIter = 0\n",
        "        for combination in combinations:\n",
        "            controlIter += 1\n",
        "\n",
        "            rest = all_target_words.copy()\n",
        "            for elem in combination:\n",
        "                rest.remove(elem)\n",
        "            all_permutations.append([list(combination), rest])\n",
        "\n",
        "    else:\n",
        "        iterations = permutation_num\n",
        "        logging.info('Too many elements for full permutation. Do random sampling. Iterations: ' + str(iterations))\n",
        "\n",
        "        for _ in itertools.repeat(None, iterations):\n",
        "            shuffle = numpy.random.permutation(all_target_words).tolist()\n",
        "            half1 = shuffle[:n // 2]\n",
        "            half2 = shuffle[n // 2:]\n",
        "            all_permutations.append([half1,half2])\n",
        "            k += 1\n",
        "        # logging.info('permutations done. number: ' + str(len(all_permutations)))\n",
        "\n",
        "    i = 0\n",
        "    n_2 = len(all_permutations)\n",
        "    higher = 0\n",
        "    equal = 0\n",
        "    lower = 0\n",
        "\n",
        "    # logging.info('permutation test: call s-corrected')\n",
        "    weat = s_corrected(words, X, Y, A, B, False)\n",
        "    # logging.info('done: call s-corrected. result: ' + str(weat))\n",
        "    plot_vals = []\n",
        "\n",
        "    if not full:\n",
        "        logging.info('betrachte Permutations-P-Werte nur im Betrag')\n",
        "    logging.info('start loop')\n",
        "    for permutation in all_permutations:\n",
        "        s_val = s_corrected(words, permutation[0], permutation[1], A, B, False)\n",
        "        if not full:\n",
        "            s_val = abs(s_val)          # Betrachte nur Samples aus dem positiven Raum und versopple so die Anzahl\n",
        "        plot_vals.append(s_val)\n",
        "        bar(i, n_2, 50, \"P value calculation: \")\n",
        "        if s_val > weat:\n",
        "            higher += 1\n",
        "        elif weat > s_val:\n",
        "            lower += 1\n",
        "        elif weat == s_val:\n",
        "            equal += 1\n",
        "        else:\n",
        "            logging.error('hier stimmt was nicht!')\n",
        "        i += 1\n",
        "\n",
        "    p_value = 1 - (lower / (higher + equal + lower))       # observed or greater difference\n",
        "    if tag:\n",
        "        plt_hist(plot_vals, tag, weat)\n",
        "    return weat, p_value, higher, equal, lower\n",
        "\n",
        "\n",
        "def plt_hist(list, name, weat=None, bins=30):\n",
        "    \"\"\"\n",
        "    plot permutation stats as histogram to visualise significance of result. Automatically saves the plot\n",
        "    :param list:        list of weat-values to plot\n",
        "    :param name:        (string) name for log file (no file extension)\n",
        "    :param weat:        (number) actual weat value\n",
        "    :param bins:        bin size for histograms\n",
        "    \"\"\"\n",
        "    plt.hist(list, bins)\n",
        "    list.sort()\n",
        "    if weat:\n",
        "        plt.axvline(weat, color='r')\n",
        "    #print(list)\n",
        "\n",
        "    plt.title('permutation test values')\n",
        "    plt.xlabel('value')\n",
        "    plt.savefig('./plots/plt_' + name + '.pdf')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bvCFUOF57Z3x"
      },
      "source": [
        "Call function executes the whole WEAT method for a certain set up.\n",
        "model : pretrained W2V model; X,Y : Target sets; A,B : Association sets.\n",
        "\n",
        "Hier ist ein bisschen Chaos, weil ich mir alle möglichen Sachen hab ausgeben lassen. Die erste der beiden call funktionen hat p-test signifikanzen enthalten. Die zweite ist etwas mehr basic."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZV35Irbf7Z3x"
      },
      "outputs": [],
      "source": [
        "def call(model, X, Y, A, B, tag='none', lower_case=False, permutation_num=5000):\n",
        "\n",
        "  # Convert the Word2Vec object to a list of words\n",
        "  words = list(model.wv.index_to_key)\n",
        "\n",
        "\n",
        "  if lower_case:\n",
        "    logging.info('lower vocabulary')\n",
        "    X = low(X)\n",
        "    Y = low(Y)\n",
        "    A = low(A)\n",
        "    B = low(B)\n",
        "\n",
        "    x_in = [x for x in X if x in words]\n",
        "    x_not = [x for x in X if x not in words]\n",
        "\n",
        "    y_in = [y for y in Y if y in words]\n",
        "    y_not = [y for y in Y if y not in words]\n",
        "\n",
        "    a_in = [a for a in A if a in words]\n",
        "    a_not = [a for a in A if a not in words]\n",
        "\n",
        "    b_in = [b for b in B if b in words]\n",
        "    b_not = [b for b in B if b not in words]\n",
        "\n",
        "    output = [tag + ': ']\n",
        "\n",
        "    if not (x_in and y_in and a_in and b_in):\n",
        "        output. append('one list is empty')\n",
        "    elif len(x_in) < 3 or len(y_in) < 3:\n",
        "        output.append('list to short')\n",
        "\n",
        "    else:\n",
        "        weat, p, h, e, l = permutation_test(model, x_in, y_in, a_in, b_in, tag, permutation_num)\n",
        "        es = effect_size(model, X, Y, A, B)\n",
        "\n",
        "        trans = ' '\n",
        "        if p > 0.95:\n",
        "            p = 1-p\n",
        "            es = es * (-1)\n",
        "            trans = ' T'\n",
        "\n",
        "        if p < 0.05:\n",
        "            if p < 0.01:\n",
        "                if p < 0.001:\n",
        "                    output.append(str(round(es, 4)) + trans + '***       ' + str(h+e+l))\n",
        "                else:\n",
        "                    output.append(str(round(es, 4)) + trans + '**        ' + str(h+e+l))\n",
        "            else:\n",
        "                output.append(str(round(es, 4)) + trans + '*         ' + str(h+e+l))\n",
        "        else:\n",
        "            output.append('nicht signifikant. '+ str(h+e+l))\n",
        "    output.append(' \\n')\n",
        "    print(output)\n",
        "    return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tqUVuFju7Z3x"
      },
      "outputs": [],
      "source": [
        "def call(model, X, Y, A, B, tag='none', lower_case=False, permutation_num=5000):\n",
        "    \"\"\"\n",
        "    put everything together\n",
        "    :param model:               underlying model\n",
        "    :param X:                   target set 1\n",
        "    :param Y:                   target set 2\n",
        "    :param A:                   association set 1\n",
        "    :param B:                   association set 2\n",
        "    :param tag:                 name for saving, default='none'\n",
        "    :param lower_case:\n",
        "    :param permutation_num:     max number of permutation for permutation test\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    # Convert the Word2Vec object to a list of words\n",
        "    words = list(model.wv.index_to_key)\n",
        "\n",
        "\n",
        "    if lower_case:\n",
        "        print('#########################')\n",
        "        logging.info('lower vocabulary')\n",
        "        X = low(X)\n",
        "        Y = low(Y)\n",
        "        A = low(A)\n",
        "        B = low(B)\n",
        "    x_in = [x for x in X if x in words]\n",
        "    x_not = [x for x in X if x not in words]\n",
        "    y_in = [y for y in Y if y in words]\n",
        "    y_not = [y for y in Y if y not in words]\n",
        "    a_in = [a for a in A if a in words]\n",
        "    a_not = [a for a in A if a not in words]\n",
        "    b_in = [b for b in B if b in words]\n",
        "    b_not = [b for b in B if b not in words]\n",
        "    print(x_in, y_in, a_in, b_in)\n",
        "    output = ['\\n\\n######################## ' + tag + ' ##### ' + str(lower_case) + ' ##################### \\n']\n",
        "    output.append('X : ' + str(x_in) + '\\n')\n",
        "    output.append('X negative : ' + str(x_not) + '\\n')\n",
        "    output.append('Y : ' + str(y_in) + '\\n')\n",
        "    output.append('Y negative : ' + str(y_not) + '\\n\\n')\n",
        "    output.append('A: ' + str(a_in) + '\\n')\n",
        "    output.append('A negative: ' + str(a_not) + '\\n')\n",
        "    output.append('B: ' + str(b_in) + '\\n')\n",
        "    output.append('B negative: ' + str(b_not) + '\\n\\n')\n",
        "    if not (x_in and y_in and a_in and b_in):\n",
        "        logging.error('one list is empty')\n",
        "        output.append('one list is empty!!!')\n",
        "    else:\n",
        "        weat, p, h, e, l = permutation_test(model, x_in, y_in, a_in, b_in, tag, permutation_num)\n",
        "        es = effect_size(model, X, Y, A, B)\n",
        "        output.append('weat value : ' + str(weat) + '\\n')\n",
        "        output.append('Permutation: p = ' + str(p) + ' (higher: ' + str(h) + ', equal: ' + str(e) + ', lower: ' + str(l) + ')\\n')\n",
        "        output.append('effect size: ' + str(es))\n",
        "    return output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pEjDslo57Z3x"
      },
      "source": [
        "Now you can call single WEAT calculations. Postitiv WEAT value means A to X and B to Y. Negative WEAT value means A to Y and B to X."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 445
        },
        "id": "cEfqd3Os7Z3y",
        "outputId": "1462cfd8-9773-4070-a653-bc075769b475"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'gensim.models.word2vec.Word2Vec'>\n",
            "25\n",
            "25\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:root:ungerade Anzahl von Target Words im Spiel!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['rose', 'pansy'] ['fly', 'cricket', 'moth'] ['health', 'family', 'support', 'honor', 'friend', 'honest', 'wonderful', 'peace', 'rainbow', 'positive', 'passion', 'diamond', 'glory', 'pleasure', 'lucky', 'freedom', 'strong', 'love', 'happy', 'heaven', 'gentle'] ['cancer', 'stress', 'evil', 'waste', 'jail', 'disaster', 'abuse', 'murder', 'negative', 'violent', 'poor', 'failure', 'war', 'death', 'kill', 'bad']\n",
            "5\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'list' object has no attribute 'wv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-81c5555c39fa>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwordSets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgw_flow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwordSets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgw_insec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mwordSets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgw_flow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwordSets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgw_insec\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mgeneralPos\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mgeneralNeg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'cd'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-14-2b32a2d7e1fb>\u001b[0m in \u001b[0;36mcall\u001b[0;34m(model, X, Y, A, B, tag, lower_case, permutation_num)\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'one list is empty!!!'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0mweat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpermutation_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_in\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_in\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma_in\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb_in\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtag\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpermutation_num\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m         \u001b[0mes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meffect_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mB\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'weat value : '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweat\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-12-b4ec8a55964b>\u001b[0m in \u001b[0;36mpermutation_test\u001b[0;34m(model, X, Y, A, B, tag, permutation_num)\u001b[0m\n\u001b[1;32m    261\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m     \u001b[0;31m# logging.info('permutation test: call s-corrected')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 263\u001b[0;31m     \u001b[0mweat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ms_corrected\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mB\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    264\u001b[0m     \u001b[0;31m# logging.info('done: call s-corrected. result: ' + str(weat))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[0mplot_vals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-12-b4ec8a55964b>\u001b[0m in \u001b[0;36ms_corrected\u001b[0;34m(model, X, Y, A, B, out)\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0ms_corrected\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mB\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;31m# Convert the Word2Vec object to a list of words\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m     \u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_to_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m     \"\"\"\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'wv'"
          ]
        }
      ],
      "source": [
        "import wordSets\n",
        "print(type(m))\n",
        "print(len(wordSets.gw_flow))\n",
        "print(len(wordSets.gw_insec))\n",
        "print(call(m,wordSets.gw_flow, wordSets.gw_insec,generalPos,generalNeg, 'cd'))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}