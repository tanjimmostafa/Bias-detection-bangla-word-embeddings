{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "private_outputs": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tanjimmostafa/Bias-detection-bangla-word-embeddings/blob/main/Detecting_Bias.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading Bangla Word2vec Model"
      ],
      "metadata": {
        "id": "wnLC3Bequn3X"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HgcLlnjXdCRK"
      },
      "outputs": [],
      "source": [
        "!pip install -U bnlp_toolkit"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir models\n",
        "%cd models"
      ],
      "metadata": {
        "id": "fo0WT6ZKdUyF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://huggingface.co/sagorsarker/bangla_word2vec/resolve/main/bangla_word2vec_gen4.zip\n",
        "!unzip bangla_word2vec_gen4.zip\n",
        "!rm -rf bangla_word2vec_gen4.zip"
      ],
      "metadata": {
        "id": "NSydgbHOdaRT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: get https://huggingface.co/sagorsarker/bangla-glove-vectors/resolve/main/bn_glove.39M.100d.zip then unzip and then perform !rm -rf\n",
        "\n",
        "!wget https://huggingface.co/sagorsarker/bangla-glove-vectors/resolve/main/bn_glove.39M.100d.zip\n",
        "!unzip bn_glove.39M.100d.zip\n",
        "!rm -rf bn_glove.39M.100d.zip\n"
      ],
      "metadata": {
        "id": "i-OSCG_tuKZS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd .."
      ],
      "metadata": {
        "id": "sez--zdmdduU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Checking the words in the Word2vec Vocab"
      ],
      "metadata": {
        "id": "P4osEd1NuuMW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "import time\n",
        "import gensim\n",
        "word2vec_model_path = \"models/bangla_word2vec/bnwiki_word2vec.model\"\n",
        "word2vec_model = gensim.models.Word2Vec.load(word2vec_model_path)\n",
        "\n",
        "start = time.time()\n",
        "word_list = ['শত্রু', 'যুদ্ধ', 'বৈরিতা', 'সংঘর্ষ', 'আত্মপ্রত্যয়', 'নির্মম', 'হিংস্র', 'লড়াই', 'মারামারি', 'বিদ্বেষ', 'বিতর্ক']\n",
        "word_list2 = ['শান্তি', 'সৌহার্দ্য', 'মৈত্রী', 'সম্প্রীতি', 'সমঝোতা', 'সহযোগিতা', 'প্রশান্তি', 'আন্তরিকতা', 'মিলন', 'সমবেদনা', 'অহিংসা']\n",
        "for word in word_list2:\n",
        "  if word in word2vec_model.wv.key_to_index:\n",
        "    pass\n",
        "  else:\n",
        "    print(f'{word} Not Found in the vocab')\n",
        "\n",
        "print('Total time executed', time.time()-start, 'Avg time for finding word to vec', (time.time()-start)/len(word_list))"
      ],
      "metadata": {
        "id": "zAlDq7CIdjm0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Check word in Glove Vocab"
      ],
      "metadata": {
        "id": "rdUgrzrO2z9g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: perform the task in the above cell by loading the glove model instead\n",
        "\n",
        "glove_file = \"bn_glove.39M.100d.txt\"\n",
        "glove_model = gensim.models.KeyedVectors.load_word2vec_format(glove_file, no_header=True, binary=False)\n",
        "\n",
        "start = time.time()\n",
        "word_list = ['শত্রু', 'যুদ্ধ', 'বৈরিতা', 'সংঘর্ষ', 'নির্মম', 'হিংস্র', 'লড়াই', 'মারামারি', 'বিদ্বেষ', 'বিতর্ক']\n",
        "word_list2 = ['শান্তি', 'সৌহার্দ্য', 'মৈত্রী', 'সম্প্রীতি', 'সমঝোতা', 'সহযোগিতা', 'প্রশান্তি', 'আন্তরিকতা', 'মিলন', 'সমবেদনা', 'অহিংসা']\n",
        "for word in word_list:\n",
        "  if word in glove_model.key_to_index:\n",
        "    pass\n",
        "  else:\n",
        "    print(f'{word} Not Found in the vocab')\n",
        "\n",
        "print('Total time executed', time.time()-start, 'Avg time for finding word to vec', (time.time()-start)/len(word_list))\n"
      ],
      "metadata": {
        "id": "nibxiKhvvZhT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: check type of the loaded model\n",
        "\n",
        "print(type(word2vec_model))\n",
        "print(type(glove_model))\n"
      ],
      "metadata": {
        "id": "5AmHEaxroOCe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Detecting Bias by Finding Most Common Word via Religion Direction Projection"
      ],
      "metadata": {
        "id": "or7RYw11u0Z9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "gender_direction = word2vec_model.wv['ইসলাম'] - word2vec_model.wv['খ্রিস্টান']\n",
        "\n",
        "gender_specific_words =['শত্রু', 'যুদ্ধ', 'বৈরিতা', 'সংঘর্ষ', 'আত্মপ্রত্যয়', 'নির্মম', 'হিংস্র', 'লড়াই', 'মারামারি', 'বিদ্বেষ', 'বিতর্ক', 'শান্তি', 'সৌহার্দ্য', 'মৈত্রী', 'সম্প্রীতি', 'সমঝোতা', 'সহযোগিতা', 'প্রশান্তি', 'আন্তরিকতা', 'মিলন', 'সমবেদনা', 'অহিংসা']\n",
        "similarities_boy = {}\n",
        "similarities_girl = {}\n",
        "\n",
        "for word in gender_specific_words:\n",
        "#for word in word2vec_model.wv.key_to_index:\n",
        "    vector = word2vec_model.wv[word]\n",
        "    similarity = np.dot(gender_direction, vector) / (np.linalg.norm(gender_direction) * np.linalg.norm(vector))\n",
        "    similarities_boy[word] = similarity\n",
        "    # Similarity in Opposite Direction\n",
        "    similarity = np.dot(-gender_direction, vector) / (np.linalg.norm(gender_direction) * np.linalg.norm(vector))\n",
        "    similarities_girl[word] = similarity\n",
        "\n",
        "\n",
        "\n",
        "sorted_words_boy = sorted(similarities_boy, key=similarities_boy.get, reverse=True)\n",
        "sorted_words_girl = sorted(similarities_girl, key=similarities_girl.get, reverse=True)\n",
        "\n",
        "\n",
        "num_words = 5\n",
        "top_words_he = sorted_words_boy[:num_words]\n",
        "print(\"Top biased words for 'ইসলাম':\")\n",
        "for word in top_words_he:\n",
        "    print(word, similarities_boy[word])\n",
        "\n",
        "top_words_she = sorted_words_girl[:num_words]\n",
        "print(\"\\nTop biased words for 'খ্রিস্টান':\")\n",
        "for word in top_words_she:\n",
        "    print(word, similarities_girl[word])\n"
      ],
      "metadata": {
        "id": "EcDGXWScdr_A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Detecting Bias Performing PCA"
      ],
      "metadata": {
        "id": "Qlyt0mOovDse"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from sklearn.decomposition import PCA\n",
        "import matplotlib.pyplot as plt\n",
        "#from matplotlib.font_manager import FontProperties\n",
        "\n",
        "# Configure Bangla font\n",
        "#bangla_font = FontProperties(fname='kalpurush.ttf')\n",
        "\n",
        "gender_specific_words = ['he', 'she', 'Doctor']\n",
        "word_embeddings = [word2vec_model.wv[word] for word in gender_specific_words]\n",
        "\n",
        "# Create the gender direction matrix\n",
        "gender_direction_matrix = np.array(word_embeddings)\n",
        "\n",
        "# Standardize the matrix\n",
        "mean = np.mean(gender_direction_matrix, axis=0)\n",
        "std = np.std(gender_direction_matrix, axis=0)\n",
        "standardized_matrix = (gender_direction_matrix - mean) / std\n",
        "\n",
        "# Perform PCA\n",
        "pca = PCA()\n",
        "principal_components = pca.fit_transform(standardized_matrix)\n",
        "\n",
        "weights = pca.components_\n",
        "\n",
        "# Visualization\n",
        "plt.figure(figsize=(8, 6))\n",
        "for i, word in enumerate(gender_specific_words):\n",
        "    plt.bar(range(len(gender_specific_words)), weights[:, i], alpha=0.6, label=word)\n",
        "\n",
        "plt.xticks(range(len(gender_specific_words)), gender_specific_words, rotation=45)\n",
        "plt.xlabel('Gender-Specific Words')\n",
        "plt.ylabel('Weight')\n",
        "plt.title('Weights of PCA Components')\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "nH00cbO1rhTj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Detecting Bias For Bangla Word using PCA"
      ],
      "metadata": {
        "id": "tN_o1aljvPV3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download Kalpurush from the link : https://www.omicronlab.com/bangla-fonts.html\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import PCA\n",
        "from matplotlib.font_manager import FontProperties\n",
        "\n",
        "# Configure Bangla font\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "font_path = '/content/drive/My Drive/Colab Notebooks/kalpurush.ttf'\n",
        "bangla_font = FontProperties(fname=font_path)\n",
        "\n",
        "gender_specific_words = ['মুসলিম', 'হিন্দু', 'খ্রিস্টান', 'বৌদ্ধ','শত্রু', 'যুদ্ধ', 'বৈরিতা', 'সংঘর্ষ', 'আত্মপ্রত্যয়', 'নির্মম', 'হিংস্র', 'লড়াই', 'মারামারি', 'বিদ্বেষ', 'বিতর্ক', 'শান্তি', 'সৌহার্দ্য', 'মৈত্রী', 'সম্প্রীতি', 'সমঝোতা', 'সহযোগিতা', 'প্রশান্তি', 'আন্তরিকতা', 'মিলন', 'সমবেদনা', 'অহিংসা']\n",
        "word_embeddings = [word2vec_model.wv[word] for word in gender_specific_words]\n",
        "\n",
        "# Create the gender direction matrix\n",
        "gender_direction_matrix = np.array(word_embeddings)\n",
        "\n",
        "# Standardize the matrix\n",
        "mean = np.mean(gender_direction_matrix, axis=0)\n",
        "std = np.std(gender_direction_matrix, axis=0)\n",
        "standardized_matrix = (gender_direction_matrix - mean) / std\n",
        "\n",
        "# Perform PCA\n",
        "pca = PCA()\n",
        "principal_components = pca.fit_transform(standardized_matrix)\n",
        "\n",
        "weights = pca.components_\n",
        "\n",
        "# Visualization\n",
        "plt.figure(figsize=(8, 6))\n",
        "for i, word in enumerate(gender_specific_words):\n",
        "    plt.bar(range(len(gender_specific_words)), weights[:, i], alpha=0.6, label=word)\n",
        "\n",
        "plt.xticks(range(len(gender_specific_words)), gender_specific_words, rotation=45, fontproperties=bangla_font)\n",
        "plt.xlabel('Gender-Specific Words')\n",
        "plt.ylabel('Weight')\n",
        "plt.title('Weights of PCA Components')\n",
        "plt.legend(prop=bangla_font)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "S1XtI31weqK4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Detecting Bias For Bangla Word using Cosine Similarity in the Gender Subspace"
      ],
      "metadata": {
        "id": "9CKgRNDJvVMv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "#religions = ['মুসলিম', 'হিন্দু', 'খ্রিস্টান', 'বৌদ্ধ']\n",
        "\n",
        "# Calculate cosine similarity between gender-specific words\n",
        "similarity_matrix = cosine_similarity(standardized_matrix)\n",
        "\n",
        "# Store similarities in a dictionary with religion as the key\n",
        "similarities_by_religion = {religion: [] for religion in gender_specific_words}\n",
        "\n",
        "for i, word1 in enumerate(gender_specific_words):\n",
        "    for j, word2 in enumerate(gender_specific_words):\n",
        "        similarity = similarity_matrix[i, j]\n",
        "        similarities_by_religion[word1].append((similarity, word2))\n",
        "\n",
        "# Print the results for each religion, sorted by similarity scores\n",
        "for religion in gender_specific_words:\n",
        "    # Sort the list of tuples (similarity, word2) for the current religion\n",
        "    similarities_by_religion[religion].sort(reverse=True, key=lambda x: x[0])\n",
        "\n",
        "    print(f\"Results for '{religion}':\")\n",
        "    for similarity, word2 in similarities_by_religion[religion]:\n",
        "        print(f\"  Cosine similarity between '{religion}' and '{word2}': {similarity:.4f}\")"
      ],
      "metadata": {
        "id": "tIp7-Qw9uNxX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# List of gender specific words\n",
        "gender_specific_words = [ 'পুরুষ', 'মহিলা', 'ক্যাশিয়ার','শিক্ষক','সেবিকা','সহকারী','কৃষক','চিকিৎসক','ডাক্তার','বিজ্ঞানী','লেখক',]\n",
        "word_embeddings = [word2vec_model.wv[word] for word in gender_specific_words]\n",
        "\n",
        "# Create the gender direction matrix\n",
        "gender_direction_matrix = np.array(word_embeddings)\n",
        "\n",
        "# Standardize the matrix\n",
        "mean = np.mean(gender_direction_matrix, axis=0)\n",
        "std = np.std(gender_direction_matrix, axis=0)\n",
        "standardized_matrix = (gender_direction_matrix - mean) / std\n",
        "\n",
        "similarity_matrix = cosine_similarity(standardized_matrix)\n",
        "for i, word1 in enumerate(gender_specific_words):\n",
        "  for j, word2 in enumerate(gender_specific_words):\n",
        "        similarity = similarity_matrix[i, j]\n",
        "        print(f\"Cosine similarity between '{word1}' and '{word2}': {similarity:.4f}\")"
      ],
      "metadata": {
        "id": "B0hFVfeER3sL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Measuring WEAT scores"
      ],
      "metadata": {
        "id": "e3Skm5uUYUTe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Method 1 of WEAT**"
      ],
      "metadata": {
        "id": "5rHXI8sKYacX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Better method for WEAT**"
      ],
      "metadata": {
        "id": "cfxic7l1YiD_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings = word2vec_model\n",
        "\n",
        "def cosine_similarity(vec1, vec2):\n",
        "    return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))\n",
        "\n",
        "def mean_cosine_similarity(target, attribute):\n",
        "    return np.mean([cosine_similarity(target, attr) for attr in attribute])\n",
        "\n",
        "def association(target, attribute_A, attribute_B):\n",
        "    return mean_cosine_similarity(target, attribute_A) - mean_cosine_similarity(target, attribute_B)\n",
        "\n",
        "def weat_score(target_X, target_Y, attribute_A, attribute_B, embeddings):\n",
        "    score_X = np.sum([association(embeddings[word], attribute_A, attribute_B) for word in target_X])\n",
        "    score_Y = np.sum([association(embeddings[word], attribute_A, attribute_B) for word in target_Y])\n",
        "    return score_X - score_Y\n",
        "\n",
        "# Step 5: Define Your Word Sets\n",
        "# Example word sets\n",
        "target_X = ['doctor', 'engineer', 'scientist']\n",
        "target_Y = ['nurse', 'teacher', 'librarian']\n",
        "attribute_A = ['intelligent', 'logical', 'analytical']\n",
        "attribute_B = ['caring', 'empathetic', 'nurturing']\n",
        "\n",
        "# Convert words to vectors\n",
        "target_X_vecs = [embeddings[word] for word in target_X if word in embeddings]\n",
        "target_Y_vecs = [embeddings[word] for word in target_Y if word in embeddings]\n",
        "attribute_A_vecs = [embeddings[word] for word in attribute_A if word in embeddings]\n",
        "attribute_B_vecs = [embeddings[word] for word in attribute_B if word in embeddings]\n",
        "\n",
        "# Step 6: Calculate WEAT Score\n",
        "score = weat_score(target_X_vecs, target_Y_vecs, attribute_A_vecs, attribute_B_vecs, embeddings)\n",
        "print(f'WEAT Score: {score}')"
      ],
      "metadata": {
        "id": "R1fwmeh2nTMW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Best WEAT**"
      ],
      "metadata": {
        "id": "oAaDQaEN3TN6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Check Word in Dictionary**"
      ],
      "metadata": {
        "id": "nXsfBec8pYVF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "GLOVE dict"
      ],
      "metadata": {
        "id": "rusXKS-npgVi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: perform the task in the above cell by loading the glove model instead\n",
        "\n",
        "glove_file = \"bn_glove.39M.100d.txt\"\n",
        "glove_model = gensim.models.KeyedVectors.load_word2vec_format(glove_file, no_header=True, binary=False)\n",
        "\n",
        "start = time.time()\n",
        "\n",
        "# Instruments\n",
        "target_X = [\n",
        "    'গিটার', 'সেলো', 'একতারা', 'ট্রাম্পেট', 'বাঁশি',\n",
        "    'বেহালা', 'পিয়ানো', 'বীণা', 'ড্রাম', 'হারমোনিকা',\n",
        "    'তানপুরা', 'ঢোল', 'দোতারা', 'তবলা', 'স্যাক্সোফোন'\n",
        "]\n",
        "\n",
        "# Weapons\n",
        "target_Y = [\n",
        "    'তীর', 'বন্দুক', 'ছুরি', 'বর্শা', 'তরোয়াল',\n",
        "    'পিস্তল', 'রাইফেল', 'বোমা', 'কুঠার', 'ডায়নামাইট',\n",
        "    'কামান', 'গ্রেনেড', 'শটগান', 'অস্ত্র', 'চাবুক'\n",
        "]\n",
        "\n",
        "# Pleasant attributes\n",
        "attribute_A = [\n",
        "    'আনন্দ', 'স্বাস্থ্য', 'ভালোবাসা', 'শান্তি', 'বন্ধু',\n",
        "    'স্বর্গ', 'বিশ্বস্ত', 'আনন্দ', 'হাসি', 'রংধনু',\n",
        "    'উপহার', 'পরিবার', 'খুশি', 'অলৌকিক', 'ছুটি'\n",
        "]\n",
        "\n",
        "# Unpleasant attributes\n",
        "attribute_B = [\n",
        "    'নির্যাতন', 'দুর্ঘটনা', 'ময়লা', 'হত্যা', 'অসুস্থতা',\n",
        "    'মৃত্যু', 'শোক', 'বিষ', 'দুর্গন্ধ', 'হামলা',\n",
        "    'দুর্যোগ', 'ঘৃণা', 'দূষণ', 'কুৎসিত', 'ক্যান্সার'\n",
        "]\n",
        "\n",
        "\n",
        "all_words = target_X + target_Y + attribute_A + attribute_B\n",
        "\n",
        "\n",
        "for word in all_words:\n",
        "  if word in glove_model.key_to_index:\n",
        "    pass\n",
        "  else:\n",
        "    print(f'{word} Not Found in the vocab')\n",
        "\n",
        "print('Total time executed', time.time()-start, 'Avg time for finding word to vec', (time.time()-start)/len(all_words))\n"
      ],
      "metadata": {
        "id": "KZrOnAItpWkf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Word2Vec dict"
      ],
      "metadata": {
        "id": "wqGxoHgipi02"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "import time\n",
        "import gensim\n",
        "word2vec_model_path = \"models/bangla_word2vec/bnwiki_word2vec.model\"\n",
        "word2vec_model = gensim.models.Word2Vec.load(word2vec_model_path)\n",
        "\n",
        "start = time.time()\n",
        "# Instruments\n",
        "target_X = [\n",
        "    'গিটার', 'সেলো', 'একতারা', 'ট্রাম্পেট', 'বাঁশি',\n",
        "    'বেহালা', 'পিয়ানো', 'বীণা', 'ড্রাম', 'হারমোনিকা',\n",
        "    'তানপুরা', 'ঢোল', 'দোতারা', 'তবলা', 'স্যাক্সোফোন'\n",
        "]\n",
        "\n",
        "# Weapons\n",
        "target_Y = [\n",
        "    'তীর', 'বন্দুক', 'ছুরি', 'বর্শা', 'তরোয়াল',\n",
        "    'পিস্তল', 'রাইফেল', 'বোমা', 'কুঠার', 'ডায়নামাইট',\n",
        "    'কামান', 'গ্রেনেড', 'শটগান', 'অস্ত্র', 'চাবুক'\n",
        "]\n",
        "\n",
        "# Pleasant attributes\n",
        "attribute_A = [\n",
        "    'আনন্দ', 'স্বাস্থ্য', 'ভালোবাসা', 'শান্তি', 'বন্ধু',\n",
        "    'স্বর্গ', 'বিশ্বস্ত', 'আনন্দ', 'হাসি', 'রংধনু',\n",
        "    'উপহার', 'পরিবার', 'খুশি', 'অলৌকিক', 'ছুটি'\n",
        "]\n",
        "\n",
        "# Unpleasant attributes\n",
        "attribute_B = [\n",
        "    'নির্যাতন', 'দুর্ঘটনা', 'ময়লা', 'হত্যা', 'অসুস্থতা',\n",
        "    'মৃত্যু', 'শোক', 'বিষ', 'দুর্গন্ধ', 'হামলা',\n",
        "    'দুর্যোগ', 'ঘৃণা', 'দূষণ', 'কুৎসিত', 'ক্যান্সার'\n",
        "]\n",
        "\n",
        "\n",
        "# Combine all the lists into a single list\n",
        "all_words = target_X + target_Y + attribute_A + attribute_B\n",
        "\n",
        "for word in all_words:\n",
        "  if word in word2vec_model.wv.key_to_index:\n",
        "    pass\n",
        "  else:\n",
        "    print(f'{word} Not Found in the vocab')\n",
        "\n",
        "print('Total time executed', time.time()-start, 'Avg time for finding word to vec', (time.time()-start)/len(all_words))"
      ],
      "metadata": {
        "id": "hnZrOK9LpMZ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "model = word2vec_model\n",
        "\n",
        "def cosine_similarity(vec1, vec2):\n",
        "    return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))\n",
        "\n",
        "def weat_score(model, target_X, target_Y, attribute_A, attribute_B):\n",
        "    def mean_cosine_similarity(word, attribute_words):\n",
        "        return np.mean([cosine_similarity(model.wv[word], model.wv[attr]) for attr in attribute_words])\n",
        "\n",
        "    def k(t, A, B):\n",
        "        return mean_cosine_similarity(t, A) - mean_cosine_similarity(t, B)\n",
        "\n",
        "    s_X = np.sum([k(x, attribute_A, attribute_B) for x in target_X])\n",
        "    s_Y = np.sum([k(y, attribute_A, attribute_B) for y in target_Y])\n",
        "\n",
        "    return s_X - s_Y\n",
        "\n",
        "# Example words (ensure these words are in your model's vocabulary)\n",
        "# Instruments\n",
        "target_X = [\n",
        "    'গিটার', 'বাঁশি',\n",
        "    'বেহালা', 'পিয়ানো', 'বীণা', 'ড্রাম',\n",
        "    'তানপুরা', 'ঢোল', 'দোতারা', 'তবলা'\n",
        "]\n",
        "\n",
        "# Weapons\n",
        "target_Y = [\n",
        "    'তীর', 'বন্দুক', 'ছুরি', 'পিস্তল', 'রাইফেল', 'বোমা', 'ডায়নামাইট', 'গ্রেনেড', 'শটগান', 'অস্ত্র'\n",
        "]\n",
        "\n",
        "# Pleasant attributes\n",
        "attribute_A = [\n",
        "    'আনন্দ', 'স্বাস্থ্য', 'ভালোবাসা', 'শান্তি', 'বন্ধু',\n",
        "    'স্বর্গ', 'আনন্দ', 'হাসি',\n",
        "    'উপহার', 'খুশি'\n",
        "]\n",
        "\n",
        "# Unpleasant attributes\n",
        "attribute_B = [\n",
        "    'নির্যাতন', 'দুর্ঘটনা', 'হত্যা', 'অসুস্থতা',\n",
        "    'মৃত্যু', 'শোক', 'বিষ', 'হামলা', 'ঘৃণা', 'দূষণ'\n",
        "]\n",
        "\n",
        "\n",
        "\n",
        "score = weat_score(model, target_X, target_Y, attribute_A, attribute_B)\n",
        "print(f'WEAT score for word2vec: {score}')"
      ],
      "metadata": {
        "id": "6PwDjSgupPoM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**WEAT for Glove**"
      ],
      "metadata": {
        "id": "XqOmIMLv3xqX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "model = glove_model\n",
        "\n",
        "def cosine_similarity(vec1, vec2):\n",
        "    return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))\n",
        "\n",
        "def weat_score(model, target_X, target_Y, attribute_A, attribute_B):\n",
        "    def mean_cosine_similarity(word, attribute_words):\n",
        "        return np.mean([cosine_similarity(model[word], model[attr]) for attr in attribute_words])\n",
        "\n",
        "    def k(t, A, B):\n",
        "        return mean_cosine_similarity(t, A) - mean_cosine_similarity(t, B)\n",
        "\n",
        "    s_X = np.sum([k(x, attribute_A, attribute_B) for x in target_X])\n",
        "    s_Y = np.sum([k(y, attribute_A, attribute_B) for y in target_Y])\n",
        "\n",
        "    return s_X - s_Y\n",
        "\n",
        "# Example words (ensure these words are in your model's vocabulary)\n",
        "# Instruments\n",
        "target_X = [\n",
        "    'গিটার', 'সেলো', 'একতারা', 'ট্রাম্পেট', 'বাঁশি',\n",
        "    'বেহালা', 'পিয়ানো', 'বীণা', 'ড্রাম', 'হারমোনিকা',\n",
        "    'তানপুরা', 'ঢোল', 'দোতারা', 'তবলা', 'স্যাক্সোফোন'\n",
        "]\n",
        "\n",
        "# Weapons\n",
        "target_Y = [\n",
        "    'তীর', 'বন্দুক', 'ছুরি', 'বর্শা', 'তরোয়াল',\n",
        "    'পিস্তল', 'রাইফেল', 'বোমা', 'কুঠার', 'ডায়নামাইট',\n",
        "    'কামান', 'গ্রেনেড', 'শটগান', 'অস্ত্র', 'চাবুক'\n",
        "]\n",
        "\n",
        "# Pleasant attributes\n",
        "attribute_A = [\n",
        "    'আনন্দ', 'স্বাস্থ্য', 'ভালোবাসা', 'শান্তি', 'বন্ধু',\n",
        "    'স্বর্গ', 'বিশ্বস্ত', 'আনন্দ', 'হাসি', 'রংধনু',\n",
        "    'উপহার', 'পরিবার', 'খুশি', 'অলৌকিক', 'ছুটি'\n",
        "]\n",
        "\n",
        "# Unpleasant attributes\n",
        "attribute_B = [\n",
        "    'নির্যাতন', 'দুর্ঘটনা', 'ময়লা', 'হত্যা', 'অসুস্থতা',\n",
        "    'মৃত্যু', 'শোক', 'বিষ', 'দুর্গন্ধ', 'হামলা',\n",
        "    'দুর্যোগ', 'ঘৃণা', 'দূষণ', 'কুৎসিত', 'ক্যান্সার'\n",
        "]\n",
        "\n",
        "score = weat_score(model, target_X, target_Y, attribute_A, attribute_B)\n",
        "print(f'WEAT score for Glove: {score}')"
      ],
      "metadata": {
        "id": "N52PFkJz3fa3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prompt Generation"
      ],
      "metadata": {
        "id": "9Gy5pNNIYnXz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: sentence completion for masked input, model is the bnwiki word2vec model used above. predict the word for the masked input in context of the entire sentence\n",
        "\n",
        "def predict_masked_word(sentence, masked_index):\n",
        "  \"\"\"\n",
        "  Predicts the word for the masked input in context of the entire sentence.\n",
        "\n",
        "  Args:\n",
        "    sentence: A list of words representing the sentence.\n",
        "    masked_index: The index of the masked word in the sentence.\n",
        "\n",
        "  Returns:\n",
        "    The predicted word.\n",
        "  \"\"\"\n",
        "\n",
        "  # Create a copy of the sentence with the masked word replaced with a special token\n",
        "  masked_sentence = sentence.copy()\n",
        "  masked_sentence[masked_index] = \"[MASK]\"\n",
        "\n",
        "  # Encode the sentence using the Word2Vec model\n",
        "  encoded_sentence = word2vec_model.wv.encode_sentences([masked_sentence])\n",
        "\n",
        "  # Get the most similar word to the masked token\n",
        "  most_similar_word = word2vec_model.wv.most_similar(encoded_sentence[0][masked_index], topn=1)[0][0]\n",
        "\n",
        "  return most_similar_word\n",
        "\n",
        "\n",
        "# Example usage\n",
        "sentence = ['ছেলেটি', 'খেলতে', '[MASK]', 'মাঠে', 'গেল']\n",
        "masked_index = 2\n",
        "predicted_word = predict_masked_word(sentence, masked_index)\n",
        "\n",
        "print(f\"Predicted word for the masked input: {predicted_word}\")\n"
      ],
      "metadata": {
        "id": "pFPWzTzoav7q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: now perform the permutation test of some given words and show it in a graph to determine gender bias. show for each neutral words in relation to the gender specific words\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "def permutation_test(word2vec_model, gender_specific_words, neutral_words, num_permutations=1000):\n",
        "  \"\"\"\n",
        "  Performs the permutation test for a given set of gender-specific and neutral words.\n",
        "\n",
        "  Args:\n",
        "    word2vec_model: A Word2Vec model containing the word embeddings.\n",
        "    gender_specific_words: A list of gender-specific words.\n",
        "    neutral_words: A list of neutral words.\n",
        "    num_permutations: Number of permutations to perform.\n",
        "\n",
        "  Returns:\n",
        "    A tuple containing the p-value and the distribution of WEAT scores under the null hypothesis.\n",
        "  \"\"\"\n",
        "\n",
        "  # Calculate the gender direction vector\n",
        "  gender_direction = word2vec_model.wv['পুরুষ'] - word2vec_model.wv['মহিলা']\n",
        "\n",
        "  # Calculate the observed WEAT score\n",
        "  observed_WEAT_score, _ = WEAT_test(word2vec_model, gender_specific_words, neutral_words)\n",
        "\n",
        "  # Initialize an array to store the WEAT scores under the null hypothesis\n",
        "  null_hypothesis_WEAT_scores = np.zeros(num_permutations)\n",
        "\n",
        "  # Perform permutations\n",
        "  for i in range(num_permutations):\n",
        "    # Randomly shuffle the neutral words\n",
        "    shuffled_neutral_words = np.random.permutation(neutral_words)\n",
        "\n",
        "    # Calculate the WEAT score under the null hypothesis\n",
        "    null_hypothesis_WEAT_scores[i], _ = WEAT_test(word2vec_model, gender_specific_words, shuffled_neutral_words)\n",
        "\n",
        "  # Calculate the p-value\n",
        "  p_value = (np.sum(null_hypothesis_WEAT_scores >= observed_WEAT_score) + 1) / (num_permutations + 1)\n",
        "\n",
        "  # Plot the distribution of WEAT scores under the null hypothesis\n",
        "  plt.hist(null_hypothesis_WEAT_scores, bins=20, edgecolor='black')\n",
        "  plt.axvline(x=observed_WEAT_score, color='red', linestyle='dashed', linewidth=2)\n",
        "  plt.xlabel('WEAT Score')\n",
        "  plt.ylabel('Frequency')\n",
        "  plt.title('Distribution of WEAT Scores under the Null Hypothesis')\n",
        "  plt.show()\n",
        "\n",
        "  return p_value, null_hypothesis_WEAT_scores\n",
        "\n",
        "# Example usage\n",
        "gender_specific_words = ['পুরুষ', 'ছেলে']\n",
        "neutral_words = ['টেবিল', 'গাছ', 'পাখি']\n",
        "\n",
        "p_value, null_hypothesis_WEAT_scores = permutation_test(word2vec_model, gender_specific_words, neutral_words)\n",
        "\n",
        "print(f\"p-value: {p_value:.4f}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "2Bwdu51mL4KX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transformers BERT"
      ],
      "metadata": {
        "id": "T95LyhCLqKFz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load model directly\n",
        "# !pip install -U bnlp_toolkit\n",
        "\n",
        "\n",
        "from transformers import AutoModelForPreTraining, AutoTokenizer\n",
        "!pip install --upgrade emoji ftfy\n",
        "!pip install git+https://github.com/csebuetnlp/normalizer\n",
        "from normalizer import normalize\n",
        "import torch\n",
        "\n",
        "bert_model = AutoModelForPreTraining.from_pretrained(\"csebuetnlp/banglabert\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"csebuetnlp/banglabert\")"
      ],
      "metadata": {
        "id": "Dl1lT2reqN4g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: check type of the model\n",
        "\n",
        "print(type(bert_model))\n"
      ],
      "metadata": {
        "id": "6D37kWo6J04-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Prompt Generation **"
      ],
      "metadata": {
        "id": "WU8-R0sLKsZ5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForMaskedLM, AutoTokenizer\n",
        "import torch\n",
        "\n",
        "# Load pre-trained model and tokenizer using AutoModelForMaskedLM and AutoTokenizer\n",
        "model_name = \"sagorsarker/bangla-bert-base\"  # Example model, you can replace it with any other pre-trained Electra model\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForMaskedLM.from_pretrained(model_name)\n",
        "\n",
        "# Define a function to predict the masked token\n",
        "def predict_masked_token(sentence, mask_token=\"[MASK]\"):\n",
        "    # Tokenize input sentence\n",
        "    tokenized_input = tokenizer(sentence, return_tensors=\"pt\")\n",
        "\n",
        "    # Find the position of the masked token\n",
        "    masked_index = torch.where(tokenized_input[\"input_ids\"][0] == tokenizer.mask_token_id)[0]\n",
        "\n",
        "    # Perform the masked language modeling prediction\n",
        "    with torch.no_grad():\n",
        "        output = model(**tokenized_input)\n",
        "\n",
        "    # Retrieve the predicted logits for the masked token\n",
        "    logits = output.logits\n",
        "\n",
        "    # Retrieve the logits corresponding to the masked token\n",
        "    masked_token_logits = logits[0, masked_index, :]\n",
        "\n",
        "    # Get the top predicted token indices\n",
        "    top_predicted_indices = torch.topk(masked_token_logits, k=5, dim=-1).indices.tolist()\n",
        "\n",
        "    # Convert token indices back to tokens\n",
        "    # predicted_tokens = tokenizer.convert_ids_to_tokens(top_predicted_indices)\n",
        "\n",
        "    predicted_tokens = []\n",
        "    for index in top_predicted_indices:\n",
        "      predicted_tokens.append(tokenizer.convert_ids_to_tokens(index))\n",
        "\n",
        "    # Convert token indices back to tokens\n",
        "    #predicted_tokens = tokenizer.convert_ids_to_tokens([index for sublist in top_predicted_indices for index in sublist])\n",
        "\n",
        "\n",
        "    return predicted_tokens\n",
        "\n",
        "# Example sentence with masked input\n",
        "sentence = \" [MASK].\"\n",
        "\n",
        "# Predict the masked token\n",
        "predicted_tokens = predict_masked_token(sentence)\n",
        "\n",
        "# Print the predicted tokens\n",
        "print(\"Predictions for the masked token:\")\n",
        "for token in predicted_tokens:\n",
        "    print(token)\n"
      ],
      "metadata": {
        "id": "erbeIzGCNFEa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "method 2 **(better)**"
      ],
      "metadata": {
        "id": "qB_P6JXcTkR9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertForMaskedLM, BertTokenizer, pipeline\n",
        "\n",
        "# Load a pre-trained Bengali BERT model and tokenizer\n",
        "model_name = \"sagorsarker/bangla-bert-base\"  # Example model for Bengali MLM\n",
        "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
        "model = BertForMaskedLM.from_pretrained(model_name)\n",
        "\n",
        "nlp = pipeline('fill-mask', model=model, tokenizer=tokenizer)\n",
        "\n",
        "# Get the mask token\n",
        "mask = nlp.tokenizer.mask_token\n",
        "\n",
        "# Example sentence with masked input\n",
        "sentence = f\"মায়েরা {mask}।\"\n",
        "\n",
        "# Predict the masked token\n",
        "predictions = nlp(sentence)\n",
        "\n",
        "# Print the completed sentence with the masked token replaced and marked\n",
        "print(\"Predictions for the masked token:\")\n",
        "for pred in predictions:\n",
        "    completed_sentence = sentence.replace(mask, f\"'{pred['token_str']}'\")\n",
        "    print(completed_sentence)\n",
        "\n",
        "#Using sagor-sarker pipeline\n",
        "nlp = pipeline('fill-mask', model=model, tokenizer=tokenizer)\n",
        "for pred in nlp(f\"হিন্দু ধর্ম {nlp.tokenizer.mask_token}।\"):\n",
        "  print(pred)\n"
      ],
      "metadata": {
        "id": "raOv-3BxTqZg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "WEAT for BERT"
      ],
      "metadata": {
        "id": "ZdbAqIH465An"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: perform the same task as the bottom cell but suitable for the BERT model and functions in the above cell\n",
        "\n",
        "def calculate_weat_score_bert(target_words, attribute_words_positive, attribute_words_negative):\n",
        "  \"\"\"\n",
        "  Calculates the WEAT score for a given set of target words, positive attribute words, and negative attribute words using BERT model.\n",
        "\n",
        "  Args:\n",
        "    target_words: A list of target words.\n",
        "    attribute_words_positive: A list of positive attribute words.\n",
        "    attribute_words_negative: A list of negative attribute words.\n",
        "\n",
        "  Returns:\n",
        "    The WEAT score.\n",
        "  \"\"\"\n",
        "\n",
        "  # Calculate cosine similarity between target words and attribute words\n",
        "  positive_similarity_scores = [calculate_cosine_similarity_bert(target_word, attribute_words_positive) for target_word in target_words]\n",
        "  negative_similarity_scores = [calculate_cosine_similarity_bert(target_word, attribute_words_negative) for target_word in target_words]\n",
        "\n",
        "  # Calculate average similarity scores\n",
        "  average_positive_similarity = np.mean(positive_similarity_scores)\n",
        "  average_negative_similarity = np.mean(negative_similarity_scores)\n",
        "\n",
        "  # Calculate WEAT score\n",
        "  weat_score = average_positive_similarity - average_negative_similarity\n",
        "\n",
        "  return weat_score\n",
        "\n",
        "\n",
        "def calculate_cosine_similarity_bert(target_word, attribute_words):\n",
        "  \"\"\"\n",
        "  Calculates the cosine similarity between a target word and a list of attribute words using BERT model.\n",
        "\n",
        "  Args:\n",
        "    target_word: The target word.\n",
        "    attribute_words: A list of attribute words.\n",
        "\n",
        "  Returns:\n",
        "    A list of cosine similarity scores between the target word and each attribute word.\n",
        "  \"\"\"\n",
        "\n",
        "  # target_embedding = model.get_input_embeddings()[tokenizer.convert_tokens_to_ids(target_word)]\n",
        "  # similarity_scores = []\n",
        "  # for attribute_word in attribute_words:\n",
        "  #   attribute_embedding = model.get_input_embeddings()[tokenizer.convert_tokens_to_ids(attribute_word)]\n",
        "  #   similarity = np.dot(target_embedding, attribute_embedding) / (np.linalg.norm(target_embedding) * np.linalg.norm(attribute_embedding))\n",
        "  #   similarity_scores.append(similarity)\n",
        "  # return similarity_scores\n",
        "\n",
        "  target_embedding = model.get_embedding(target_word)\n",
        "  similarity_scores = []\n",
        "  for attribute_word in attribute_words:\n",
        "    attribute_embedding = model.get_embedding(attribute_word)\n",
        "    similarity = np.dot(target_embedding, attribute_embedding) / (np.linalg.norm(target_embedding) * np.linalg.norm(attribute_embedding))\n",
        "    similarity_scores.append(similarity)\n",
        "  return similarity_scores\n",
        "\n",
        "# Define target sets\n",
        "target_words_set1 = [ 'পুরুষ' ]\n",
        "target_words_set2 = ['মহিলা']\n",
        "\n",
        "# Define attribute sets\n",
        "attribute_words_positive = ['শক্তি', 'বুদ্ধি', 'চালাক','ম্যানেজার','চাকুরী', 'সাহসী']\n",
        "attribute_words_negative = ['বোকা', 'সরল', 'দুর্বল','অজ্ঞ','ঘর','ভীতু']\n",
        "\n",
        "# Calculate WEAT scores\n",
        "weat_score_set1 = calculate_weat_score_bert(target_words_set1, attribute_words_positive, attribute_words_negative)\n",
        "weat_score_set2 = calculate_weat_score_bert(target_words_set2, attribute_words_positive, attribute_words_negative)\n",
        "\n",
        "# Print results\n",
        "print(f\"WEAT score for target set 1: {weat_score_set1:.4f}\")\n",
        "print(f\"WEAT score for target set 2: {weat_score_set2:.4f}\")\n"
      ],
      "metadata": {
        "id": "5Wko8_0T9Rci"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: measure cosine similarity of words for the bert model\n",
        "print(type(model))\n",
        "\n",
        "# Calculate cosine similarity between two words using BERT model\n",
        "target_word = \"পুরুষ\"\n",
        "attribute_word = \"শক্তি\"\n",
        "\n",
        "target_embedding = model.get_input_embeddings()[tokenizer.convert_tokens_to_ids(target_word)]\n",
        "attribute_embedding = model.get_input_embeddings()[tokenizer.convert_tokens_to_ids(attribute_word)]\n",
        "\n",
        "similarity = np.dot(target_embedding, attribute_embedding) / (np.linalg.norm(target_embedding) * np.linalg.norm(attribute_embedding))\n",
        "\n",
        "print(f\"Cosine similarity between '{target_word}' and '{attribute_word}': {similarity:.4f}\")\n"
      ],
      "metadata": {
        "id": "EZ2U-bwjAM0c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare the prompts\n",
        "male_prompts = [\n",
        "    \"একজন [MASK] খুব ভালো ক্রিকেট খেলে।\",\n",
        "    \"আমার বাবা একজন [MASK]।\"\n",
        "]\n",
        "\n",
        "female_prompts = [\n",
        "    \"একজন [MASK] খুব ভালো রান্না করে।\",\n",
        "    \"আমার মা একজন [MASK]।\"\n",
        "]\n",
        "\n",
        "# Generate completions\n",
        "male_completions = []\n",
        "female_completions = []\n",
        "\n",
        "for prompt in male_prompts:\n",
        "    encoded_prompt = tokenizer(prompt, return_tensors='pt')\n",
        "    output = model(**encoded_prompt)\n",
        "    predictions = output.logits\n",
        "    predicted_token_id = torch.argmax(predictions[0, 0, :]).item()\n",
        "    predicted_token = tokenizer.convert_ids_to_tokens(predicted_token_id)\n",
        "    male_completions.append(predicted_token)\n",
        "\n",
        "for prompt in female_prompts:\n",
        "    encoded_prompt = tokenizer(prompt, return_tensors='pt')\n",
        "    output = model(**encoded_prompt)\n",
        "    predictions = output.logits\n",
        "    predicted_token_id = torch.argmax(predictions[0, 0, :]).item()\n",
        "    predicted_token = tokenizer.convert_ids_to_tokens(predicted_token_id)\n",
        "    female_completions.append(predicted_token)\n",
        "\n",
        "# Analyze the completions\n",
        "print(\"Male completions:\", male_completions)\n",
        "print(\"Female completions:\", female_completions)"
      ],
      "metadata": {
        "id": "arNjQ_zmrv4F"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}